.ssh_setup:
  before_script:
    - sudo mkdir -p ~/.ssh
    - echo "$THE_WAY" | base64 --decode | sudo tee ~/.ssh/the_way > /dev/null
    - sudo chmod 600 ~/.ssh/the_way

stages:
 - destroy_job
 - up_only_job
 - skip_up_job
 - validate_cluster
 - restore_cluster
 - terraform_apply

destroy_job:
  extends: .ssh_setup
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: destroy_job
  tags:
    - linux
  script:
    - sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET 'cd Documents/src/test && vagrant destroy -f'

up_only_job:
  extends: .ssh_setup
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: up_only_job
  tags:
    - linux
  timeout: 3h
  script:
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T Make_Kubernetes.sh $TEST_TARGET:Documents/src/test/Make_Kubernetes.sh
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T config.json $TEST_TARGET:Documents/src/test/config.json
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T Vagrantfile.template $TEST_TARGET:Documents/src/test/Vagrantfile.template
    - sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET 'cd Documents/src/test && ./Make_Kubernetes.sh --location vagrant UP_ONLY'

skip_up_job:
  extends: .ssh_setup
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: skip_up_job
  tags:
    - linux
  timeout: 3h
  script:
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T hosts.template $TEST_TARGET:Documents/src/test/hosts.template
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T addons.yml $TEST_TARGET:Documents/src/test/addons.yml
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T build_inventory.template $TEST_TARGET:Documents/src/test/build_inventory.template
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T systemd/service-monitor.service $TEST_TARGET:Documents/src/test/service-monitor.service
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T systemd/service-monitor.sh $TEST_TARGET:Documents/src/test/service-monitor.sh
    - echo "sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET \"export OPENAI_API_KEY=$OPENAI_API_KEY && cd Documents/src/test && ./Make_Kubernetes.sh --location vagrant SKIP_UP\"" > /tmp/dynamics_action.sh
    - sh /tmp/dynamics_action.sh

validate_cluster:
  extends: .ssh_setup
  retry:
    max: 2
    when:
      - script_failure
      - api_failure
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: validate_cluster
  tags:
    - linux
  timeout: 1h
  script:
    # output config.json to log for ref
    - cat config.json
    # Copy validation script and config.json to the target machine
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T validate_cluster.rb config.json $TEST_TARGET:Documents/src/test/
    # Execute the validation script using ruby, reading node_name from config.json
    - |
      sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET '
        cd Documents/src/test && 
        NODE_NAME=$(jq -r .node_name config.json)1 &&
        vagrant ssh -c "ruby /vagrant/validate_cluster.rb|tee /vagrant/validate_cluster.log" $NODE_NAME
      '
    # Copy validation log back from the target machine to runner
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET:Documents/src/test/validate_cluster.log validate_cluster.log
    # Check log for fail
    - if grep "Kubernetes cluster validation passed successfully!" validate_cluster.log ; then echo "Kubernetes cluster validation passed successfully!"; else echo "Kubernetes cluster validation failed!"; exit 1; fi
  artifacts:
    paths:
      - validate_cluster.log
  dependencies:
    - up_only_job

restore_cluster:
  extends: .ssh_setup
  retry:
    max: 2
    when:
      - script_failure
      - api_failure
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: restore_cluster
  tags:
    - linux
  timeout: 1h
  script:
    # Copy the backup restore script to the target
    - sudo scp -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T Backup_Restore.sh config.json $TEST_TARGET:Documents/src/test/
    # Temp install jq, should be in runner image build
    - sudo apt-get update && sudo apt-get install -y jq
    # Get name of first node
    - export NODE_NAME="$(jq -r '.node_name' config.json)1"
    # Make directory for postgres pv
    - sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET "cd Documents/src/test/ && vagrant ssh -c 'ssh -o StrictHostKeyChecking=no machine2  mkdir -p /home/vagrant/k8s_postgres/postgresql-data' $NODE_NAME"
    # Execute backup restore script
    - sudo ssh -o StrictHostKeyChecking=no -i ~/.ssh/the_way -T $TEST_TARGET "cd Documents/src/test/ && chmod +x Backup_Restore.sh && vagrant ssh -c 'cd /vagrant && ./Backup_Restore.sh' $NODE_NAME"
  dependencies:
    - validate_cluster
  allow_failure: true

#    - NODE1_IP=$(ssh node1 "qm guest exec 200 -- ip -4 -j addr show" | jq -r '.["out-data"] | fromjson | map(.addr_info[] | select(.family == "inet") | .local) | map(select(. != "127.0.0.1")) | last')
#    - NODE2_IP=$(ssh node2 "qm guest exec 400 -- ip -4 -j addr show" | jq -r '.["out-data"] | fromjson | map(.addr_info[] | select(.family == "inet") | .local) | map(select(. != "127.0.0.1")) | last')
#    - NODE3_IP=$(ssh node3 "qm guest exec 600 -- ip -4 -j addr show" | jq -r '.["out-data"] | fromjson | map(.addr_info[] | select(.family == "inet") | .local) | map(select(. != "127.0.0.1")) | last')
#    - NODE4_IP=$(ssh node4 "qm guest exec 800 -- ip -4 -j addr show" | jq -r '.["out-data"] | fromjson | map(.addr_info[] | select(.family == "inet") | .local) | map(select(. != "127.0.0.1")) | last')

terraform_apply:
  retry:
    max: 2
    when:
      - script_failure
      - api_failure
  image: docker.ellisbs.co.uk:5190/ubuntu:24.04
  stage: terraform_apply
  tags:
    - linux
  timeout: 1h
  script:
    - if ! curl -f http://$NODE1:8006 > /dev/null 2>&1; then
        echo "Sorry proxmox not available";
        exit 0;
      fi
    - apt-get update
    - DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata docker-compose
    - cd terraform
    - echo "MY_PROJECT_TOKEN=${MY_PROJECT_TOKEN}" >> .env
    - echo "CI_JOB_TOKEN=${CI_JOB_TOKEN}" >> .env
    - echo "CI_SERVER_HOST=${CI_SERVER_HOST}" >> .env
    - echo "CI_PROJECT_PATH=${CI_PROJECT_PATH}" >> .env
    - echo "CI_COMMIT_SHA=${CI_COMMIT_SHA}" >> .env
    - docker-compose up -d
    - docker-compose exec -T opentofu sh -c "
        cd /opt/pwd &&
        rm -rf vagrant_kubernetes &&
        export CI_JOB_TOKEN=${MY_PROJECT_TOKEN}  &&
        export CI_SERVER_HOST=gitlab.ellisbs.co.uk  &&
        export CI_PROJECT_PATH=ian/vagrant_kubernetes  &&
        export CI_COMMIT_SHA=${CI_COMMIT_SHA}  &&
        git config --global http.sslVerify false  &&
        git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ellisbs.co.uk/${CI_PROJECT_PATH}.git  &&
        cd vagrant_kubernetes  &&
        git checkout ${CI_COMMIT_SHA}  &&
        cd terraform  &&
        tofu init  &&
        tofu validate &&
        tofu apply --auto-approve
      "
